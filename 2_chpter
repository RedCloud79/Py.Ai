## 텐서플로우로 딥러닝 구현하기

# 1입력 1출력 ( 'numpy'라는 행렬을 활용 )
* X(입력값), T(목표 값), W(가중치), B(편향)
* tensorflow = Sequential( 순차적으로 처리를 해주는 신경망 )
* optimizer = 아담(Adam)의 활용도(성능)가 높다.
* W,B의 값을 건들경우는 거이 없다.
* model.save() 로 외부에 저장하는 방식이 보통.
* cnn의 신경망 형태로 할 경우 학습률이 올라간다.

# 1입력 1출력 인공신경
ex_1) dnn_test (기본적인 틀 코드)
import tensorflow as tf
import numpy as np

X = np.array([[2]])
YT = np.array([[10]])
W=np.array([[3]])
B=np.array([1])

model = tf.keras.Sequential([
    tf.keras.Input(shape=(1,)),
    tf.keras.layers.Dense(1)
])

model.layers[0].set_weights([W,B])

model.compile(optimizer = 'sgd',
              loss='mse')

Y = model.predict(X)
print(Y)

model.fit(X,YT,epochs=1000)

print('W=',model.layers[0].get_weights()[0])
print('B=',model.layers[0].get_weights()[1])

Y = model.predict(X)
print(Y)
* 결과값
W= [[4.1999974]]
B= [1.6000019]
1/1 [==============================] - 0s 16ms/step
[[9.999997]]


# 2입력 1출력의 예제
ex_1)
import tensorflow as tf
import numpy as np

X = np.array([[2,3]])
YT = np.array([[27]])
W=np.array([[3],[4]])
B=np.array([1])

model = tf.keras.Sequential([
    tf.keras.Input(shape=(2,)),
    tf.keras.layers.Dense(1)
])

model.layers[0].set_weights([W,B])

model.compile(optimizer = 'sgd',
              loss='mse')

Y = model.predict(X)
print(Y)

model.fit(X,YT,epochs=1000)

print('W=',model.layers[0].get_weights()[0])
print('B=',model.layers[0].get_weights()[1])

Y = model.predict(X)
print(Y)
* 결과값
W= [[4.1428566]
 [5.714285 ]]
B= [1.5714294]
1/1 [==============================] - 0s 16ms/step
[[26.999998]]

# 2입력 2출력의 예제
ex_1)
import tensorflow as tf
import numpy as np

X = np.array([[2,3]])
YT = np.array([[27, -30]])
W=np.array([[3,5],[4,6]])
B=np.array([1,2])

model = tf.keras.Sequential([
    tf.keras.Input(shape=(2,)),
    tf.keras.layers.Dense(2)
])

model.layers[0].set_weights([W,B])

model.compile(optimizer = 'sgd',
              loss='mse')

Y = model.predict(X)
print(Y)

model.fit(X,YT,epochs=1000)

print('W=',model.layers[0].get_weights()[0])
print('B=',model.layers[0].get_weights()[1])

Y = model.predict(X)
print(Y)
* 결과값
W= [[ 4.1428556 -3.5714262]
 [ 5.714284  -6.857143 ]]
B= [ 1.5714294 -2.2857132]
1/1 [==============================] - 0s 16ms/step
[[ 26.999994 -29.999994]]


# 2입력 2은닉 2출력의 예제
ex_1)
import tensorflow as tf
import numpy as np

X = np.array([[.05,.10]])
YT = np.array([[.01, .99]])
W=np.array([[.15,.25],[.20,.30]])
B=np.array([.35,.35])
W2=np.array([[.40,.50],[.45,.55]])
B2=np.array([.60,.60])

model = tf.keras.Sequential([
    tf.keras.Input(shape=(2,)),
    tf.keras.layers.Dense(2),
    tf.keras.layers.Dense(2)
])

model.layers[0].set_weights([W,B])
model.layers[1].set_weights([W2,B2])

model.compile(optimizer = 'sgd',
              loss='mse')

Y = model.predict(X)
print(Y)

model.fit(X,YT,epochs=1000)

print('W=',model.layers[0].get_weights()[0])
print('B=',model.layers[0].get_weights()[1])
print('W2=',model.layers[1].get_weights()[0])
print('B2=',model.layers[1].get_weights()[1])

Y = model.predict(X)
print(Y)
* 결과값
W= [[0.14315726 0.24180055]
 [0.18631484 0.2836011 ]]
B= [0.2131526  0.18600278]
W2= [[0.20264395 0.53349453]
 [0.2525947  0.5828034 ]]
B2= [-0.09561112  0.73054373]
1/1 [==============================] - 0s 16ms/step
[[0.01000983 0.9899955 ]]


## 딥러닝 활용 맛보기
- 손글씨 숫자 데이터를 입력받아 학습을 수행하는 예제
- 28*28*1*60000개 : 훈련용, 28*28*1*10000개 : 검증용
- 입력 = input layer 784, 은닉 = hidden layer 128(relu), 출력 = output layer 10(softmax)
- Loss Layer (cross-entropy) 오차 함수

# 활용의 예시
ex_1) 손글씨 숫자 인식 예제
import tensorflow as tf

mnist = tf.keras.datasets.mnist # fashion_mnist 의류와 관련된 데이터

(X, YT), (x, yt) = mnist.load_data()
# print(X.shape, YT.shape, x.shape, yt.shape,) = 모양을 찍어보는 코드

# pip3 install matplotlib 다운로드 필요
from matplotlib import pyplot as plt
plt.imshow(X[0])
plt.show()
print(YT[0]) # YT의 값 출력
print(tf.one_hot(YT[0], 10)) * 출력값 : tf.Tensor([0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], shape=(10,), dtype=float32)

--------------------------------------------- # 숫자로 데이터모양을 표기
for row in range(28):
    for col in range(28):
        print('%4s' %X[0][row][col], end='')
    print()
    
import numpy as np

plt.figure(figsize=(10,10)) # 가로5 세로5의 데이터를 불러옴
for i in range(25):
    plt.subplot(5,5,i+1) # 세로, 가로 = 5
    plt.xticks([])
    plt.yticks([])
    plt.imshow(X[i],cmap=plt.cm.binary) #binary = 128기준으로 크면 검정 작으면 흰색
    plt.xlabel(YT[i])
plt.show()
----------------------------------------------

X, x = X/255, x/255
X, x = X.reshape((60000,784)), x.reshape((10000,784)) # 데이터를 핌
# X, x = X.reshape((60000,28,28,1)), x.reshape((10000,28,28,1)) # CNN의 경우 수정
# 28*28 => 784 :: (28,28) => (784,) DNN 2차 배열 => 1차 배열
# 28*28*1 :: (28, 28) => (28,28,1) CNN 2 차배열 => 3차배열
print(X.shape)

-------------------------------------------------
* CNN의 경우 코드 3차원 배열
# input_shape = (28,28,1)
model = tf.keras.Sequential([ # CNN의 경우 수정
    tf.keras.Input(shape=(28,28,1)),
    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'), # relu = 활성화함수
    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation='softmax'),    #  = 활성화함수
    ]) # 신경망 모양
    
    
* CNN의 경우 결과값
Epoch 5/5
1875/1875 [==============================] - 54s 29ms/step - loss: 0.0131 - accuracy: 0.9954 # 테스트
313/313 [==============================] - 2s 7ms/step - loss: 0.0542 - accuracy: 0.9863 # 실질적 테스트값
----------------------------------------------------

model = tf.keras.Sequential([ 
    tf.keras.Input(shape=(784,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax'),
    ]) # 신경망 모양

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']) # 오차 함수, metrics 얼마나 맞추는지

model.fit(X,YT,epochs=5)

model.evaluate(x,yt) # predict도 사용가능
model.save('model.h5') # .h5 라는 확장자를 사용하여 저장

* 결과값
1875/1875 [==============================] - 3s 2ms/step - loss: 0.0443 - accuracy: 0.9862
313/313 [==============================] - 0s 901us/step - loss: 0.0796 - accuracy: 0.9751


# 학습한 모델파일을 활용 한 예시
ex_1)
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(_,_),(x,yt) = mnist.load_data()

x=x/255
x=x.reshape((10000,784))

model = tf.keras.models.load_model('model.h5')

y = model.predict(x) # 한개만 추출할경우 y = model.predict(x[0:1])

print(y[0], yt[0])

import numpy as np

print(np.argmax(y[0]))

import matplotlib.pyplot as plt

x= x.reshape((10000,28,28))
plt.imshow(x[0])
plt.show()

plt.figure(figsize=(10,10)) # 출력
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x[i],cmap=plt.cm.binary)
    plt.xlabel(np.argmax(y[i]))
plt.show()

------------------------------------------------
*틀린 것의 유무 확인 코드
cnt_wrong=0
y_wrong=[]
for i in range(10000):
    if np.argmax(y[i])!=yt[i]:
        y_wrong.append(i)
        cnt_wrong += 1
print(cnt_wrong)
print(y_wrong[:10])

* 인식못한 숫자 출력
plt.imshow(x[115])
plt.show()
print(np.argmax(y[115])), yt[115]
------------------------------------------------
* 인식못한 숫자를 5,5 싸이즈로 출력
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x[y_wrong[i]],cmap=plt.cm.binary)
    plt.xlabel(f'{y_wrong[i]} : y{np.argmax(y[y_wrong[i]])} yt{yt[y_wrong[i]]}')
plt.show()
------------------------------------------------
*결과값
313/313 [==============================] - 0s 801us/step
[1.8032985e-07 1.7523572e-06 6.3887287e-06 2.1909325e-05 8.7815877e-08
 3.5283490e-06 1.4294808e-08 9.9996412e-01 3.9944635e-07 1.6487955e-06] 7
7
222
[115, 121, 199, 247, 259, 274, 321, 391, 445, 448]
8 4


## 활성화 함수
- 복잡한 패턴을 학습하게 해준다. 출력 값을 어떤 범위로 제한할 수도 있다.
- 신경 말단에서 다음 신경으로 전달될 신호를 결정하는 시냅스와 같은 역할을 한다.
- 활성화 함수의 유무에 따라 학습의 동작 유무와 학습률에 영향을 준다.

# 활성화 함수의 종류
* 시그모이드 = 0과 1로 분류하는 이진 부류 문제에 사용된다.

** ReLU = y = {x(x>0)  (-3<= x <= 3), 주로 사용되는 함수이다. CNN에서 많이 사용된다.
             {0(x<= 0)
             노드가 많을 수록 신경망 전체적으로 학습이 되지 않는 단점이 있다.
             가중치를 0에 가까운 값으로 주어 단점을 보완한다.

* adam = 학습 함수

** 소프트맥스(softmax) = (분류 회귀)모든 값을 합하면 1의 값을 만든다. 확률의 용도로 만들어 두었다.
* 회귀 = 어떠한 목표값으로 만들려고 하는 성질

# 순전파와 역전파
* sigmoid = 순 : y' = a/(1+exp(-y)), 역 : yE = y'*(1-y')*y'E

* ReLU = 순 : h'=(h>0)?h:0, 역 : hE=(h'>0)?h'E:0

# RELU 와 sigmoid
ex_1)
from math import exp
x1, x2 = 0.05, 0.10

w1, w2, w3, w4 = 0.15, 0.20, 0.25, 0.30
b1, b2 = 0.35, 0.35

w5, w6, w7, w8 = 0.40, 0.45, 0.50, 0.55
b3, b4 = 0.60, 0.60

y1T, y2T = 0.01, 0.99

lr = 0.01

EPOCH = 1000

for epoch in range(EPOCH):
    h1 = x1*w1 + x2*w2 + 1*b1 #1
    h2 = x1*w3 + x2*w4 + 1*b2
    h1 = h1 if h1>0 else 0 # RELU feed forward
    h2 = h2 if h2>0 else 0
    
    y1 = h1*w5 + h2*w6 + 1*b3
    y2 = h1*w7 + h2*w8 + 1*b4
    y1 = 1/(1+exp(-y1)) # sigmoid feed forward
    y2 = 1/(1+exp(-y2))
    
    E = ((y1-y1T)**2 + (y2-y2T)**2)/2 #2
    
    y1E = y1 - y1T # 3
    y2E = y2 - y2T
    y1E=y1*(1-y1)*y1E # sigmoid back propagation
    y2E=y1*(1-y2)*y2E
    
    h1E = y1E*w5 + y2E*w7 #4
    h2E = y1E*w6 + y2E*w8
    h1E = h1E if h1>0 else 0 # RELU back propagation
    h2E = h2E if h2>0 else 0
    
    w1E = h1E*x1 #6
    w2E = h1E*x2
    w3E = h2E*x1
    w4E = h2E*x2
    w5E = y1E*h1
    w6E = y1E*h2
    w7E = y2E*h1
    w8E = y2E*h2
    b1E = h1E*1
    b2E = h2E*1
    b3E = y1E*1
    b4E = y2E*1
    
    w1 -= lr*w1E #7
    w2 -= lr*w2E
    w3 -= lr*w3E
    w4 -= lr*w4E
    b1 -= lr*b1E
    b2 -= lr*b2E
    
    w5 -= lr*w5E
    w6 -= lr*w6E
    w7 -= lr*w7E
    w8 -= lr*w8E
    b3 -= lr*b3E
    b4 -= lr*b4E
    
    if epoch % 100 == 99:
        print(f'epoch = {epoch}')
        print(f' y1 :  {y1:.6f}')
        print(f' y2 :  {y2:.6f}')
    
    
    if E < 0.0000001 :
        break
    
print(f'w1,w3 = {w1:.6f},{w3:.6f}')
print(f'w2,w4 = {w2:.6f},{w4:.6f}')
print(f'b1,b2 = {b1:.6f},{b2:.6f}')
print(f'w5,w7 = {w5:.6f},{w7:.6f}')
print(f'w6,w8 = {w6:.6f},{w8:.6f}')
print(f'b3,b4 = {b3:.6f},{b4:.6f}')

* 결과값
epoch = 999
 y1 :  0.361699
 y2 :  0.777316
w1,w3 = 0.145834,0.243652
w2,w4 = 0.191668,0.287305
b1,b2 = 0.266676,0.223047
w5,w7 = 0.034923,0.594164
w6,w8 = 0.091842,0.642917
b3,b4 = -0.603410,0.906482


# 출력층에 softmax 적용해 보기
- 확률의 형태로 변형을 하겠다는 의미 ( 분류 회귀의 사용 )
- softmax 활성화 함수는 cross entropy error( 크로스 엔트로피 오차 ) 함수와 같이 사용된다. 'mse' 사용x
- softmax는 역전파의 형태가 따로 존재하지 않는다.

ex_1) softmax의 함수 구현
from math import exp as e
(e(1.3), e(5.1), e(2.2), e(0.7), e(1.1))
결과값 : (3.6692966676192444, 164.0219072999017, 9.025013499434122, 2.0137527074704766, 3.0041660239464334)
sumY = e(1.3) + e(5.1) + e(2.2) + e(0.7) + e(1.1)
sumY
결과값 : 181.73413619837197
(e(1.3)/sumY, e(5.1)/sumY, e(2.2)/sumY, e(0.7)/sumY, e(1.1)/sumY)
결과값 : (0.020190464732580685, 0.9025376890165726, 0.04966052987196013,
0.011080761983386346, 0.01653055439550022)


