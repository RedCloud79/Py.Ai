## softmax 함수 분모 크기 줄이기

ex_2) 가장 큰 값을 빼서 구하기 softmax
(e(1.3-5.1),e(5.1-5.1),e(2.2-5.1),e(0.7-5.1),e(1.1-5.1),)
결과값 : (0.0223707718561656, 1.0, 0.05502322005640726, 0.012277339903068448, 0.01831563888873419)
sumY = (e(1.3-5.1)+e(5.1-5.1)+e(2.2-5.1)+e(0.7-5.1)+e(1.1-5.1))
sumY
결과값 : 1.1079869707043757
(e(1.3-5.1)/sumY,e(5.1-5.1)/sumY,e(2.2-5.1)/sumY,e(0.7-5.1)/sumY,e(1.1-5.1)/sumY)
결과값 : (0.020190464732580682, 0.9025376890165725, 0.049660529871960124,
0.011080761983386346, 0.01653055439550022)

# cross entropy 오차 구현해 보기
- softmax가 오면 무조껀 같이 오는 함수 이다.

ex_1)
from math import log as ln
-1*ln(0.9)
결과값 : 0.10536051565782628
-1*ln(0.99)
결과값 : 0.01005033585350145
-1*ln(0.999)
결과값 : 0.0010005003335835344
-1*ln(0.9999)
결과값 : 0.00010000500033334732
-1*ln(0.99999)
결과값 : 1.0000050000287824e-05


ex_2) crossentropy와 softmax예제
import tensorflow as tf
import numpy as np

X=np.array([[.05,.10]]) # 입력데이터 <=
YT=np.array([[0,1]]) # 목표데이터(라벨)
W=np.array([[.15,.25],[.20,.30]]) # 가중치 <=
B=np.array([.35,.35]) # 편향
W2=np.array([[.40,.50],[.45,.55]]) # 가중치 <=
B2=np.array([.60,.60]) # 편향

model=tf.keras.Sequential([
    tf.keras.Input(shape=(2,)),
    tf.keras.layers.Dense(2, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
]) # 신경망 모양 결정(W, B 내부적 준비)

model.layers[0].set_weights([W,B])
model.layers[1].set_weights([W2,B2])

model.compile(optimizer='sgd',# 7공식, 학습
                loss='categorical_crossentropy') # 2공식, 오차계산

Y=model.predict(X)
print(Y)

model.fit(X,YT,epochs=100000)

print('W=',model.layers[0].get_weights()[0])
print('B=',model.layers[0].get_weights()[1])
print('W2=',model.layers[1].get_weights()[0])
print('B2=',model.layers[1].get_weights()[1])

Y=model.predict(X)
print(Y)
* 결과값
W= [[0.20979609 0.31164446]
 [0.31959233 0.42337626]]
B= [1.5460793 1.5839566]
W2= [[-0.6355431   1.5354836 ]
 [-0.62160754  1.6215547 ]]
B2= [-0.7672052  1.9672418]
1/1 [==============================] - 0s 16ms/step
[[5.1897896e-05 9.9994814e-01]]


ex_3) _7seg_data
import numpy as np

np.set_printoptions(precision=1, suppress=True)

X=np.array([
    [ 1, 1, 1, 1, 1, 1, 0 ], # 0
    [ 0, 1, 1, 0, 0, 0, 0 ], # 1
    [ 1, 1, 0, 1, 1, 0, 1 ], # 2
    [ 1, 1, 1, 1, 0, 0, 1 ], # 3
    [ 0, 1, 1, 0, 0, 1, 1 ], # 4
    [ 1, 0, 1, 1, 0, 1, 1 ], # 5
    [ 0, 0, 1, 1, 1, 1, 1 ], # 6
    [ 1, 1, 1, 0, 0, 0, 0 ], # 7
    [ 1, 1, 1, 1, 1, 1, 1 ], # 8
    [ 1, 1, 1, 0, 0, 1, 1 ]  # 9
])

YT=np.array([
    [ 0, 0, 0, 0 ],
    [ 0, 0, 0, 1 ],
    [ 0, 0, 1, 0 ],
    [ 0, 0, 1, 1 ],
    [ 0, 1, 0, 0 ],
    [ 0, 1, 0, 1 ],
    [ 0, 1, 1, 0 ],
    [ 0, 1, 1, 1 ],
    [ 1, 0, 0, 0 ],
    [ 1, 0, 0, 1 ]
])

ex_4) _461
from _7seg_data import X, YT

print(X.shape)
print(YT.shape)
*결과값
(10, 7)
(10, 4)


ex_5) _461_2
import tensorflow as tf
from _7seg_data import X, YT

model = tf.keras.Sequential([
    tf.keras.Input(shape=(7,)),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(4, activation='sigmoid')
    ])

model.compile(optimizer='adam', loss='mse') # mse는 평균 제곱 오차 함수

model.fit(X,YT, epochs=10000)

Y=model.predict(X)
print(Y)

*결과값
1/1 [==============================] - 0s 62ms/step
[[0. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [0. 0. 1. 1.]
 [0. 1. 0. 0.]
 [0. 1. 0. 1.]
 [0. 1. 1. 0.]
 [0. 1. 1. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 1.]]


# 국소해의 문제 해결해 보기
- 최소값 지점을 찾지 못하고 극소값 지점에 수렴하는 경우이다.
- 재학습을 수행해보거나 은닉층의 노드수를 변경을 해준다.


ex_6) _461_6 (입력과 출력을 바꿔서 실행)
import tensorflow as tf
from _7seg_data import X, YT

X, YT = YT, X

model = tf.keras.Sequential([
    tf.keras.Input(shape=(4,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(7, activation='linear')
    ])

model.compile(optimizer='adam', loss='mse')

model.fit(X,YT, epochs=10000)

Y=model.predict(X)
print(Y)
* 결과값
1/1 [==============================] - 0s 0s/step - loss: 1.4213e-10
1/1 [==============================] - 0s 62ms/step
[[ 1.  1.  1.  1.  1.  1.  0.]
 [ 0.  1.  1.  0. -0.  0. -0.]
 [ 1.  1.  0.  1.  1. -0.  1.]
 [ 1.  1.  1.  1. -0.  0.  1.]
 [ 0.  1.  1.  0. -0.  1.  1.]
 [ 1. -0.  1.  1. -0.  1.  1.]
 [ 0. -0.  1.  1.  1.  1.  1.]
 [ 1.  1.  1. -0. -0.  0.  0.]
 [ 1.  1.  1.  1.  1.  1.  1.]
 [ 1.  1.  1.  0. -0.  1.  1.]]
 
 
 ex_7) 학습 내용 저장하기
 import tensorflow as tf
from _7seg_data import X, YT


model = tf.keras.Sequential([
    tf.keras.Input(shape=(7,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(4, activation='linear')
    ])

model.compile(optimizer='adam', loss='mse')

model.fit(X,YT, epochs=10000)

Y=model.predict(X)
print(Y)

model.save('model_1.h5')


ex_8) 모델 불러와 예측하기
import tensorflow as tf
from _7seg_data import X, YT

model = tf.keras.models.load_model('model_1.h5')

Y=model.predict(X)
print(Y)

*결과값
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
1/1 [==============================] - 0s 78ms/step
[[-0. -0.  0.  0.]
 [ 0. -0.  0.  1.]
 [-0. -0.  1.  0.]
 [-0. -0.  1.  1.]
 [ 0.  1.  0.  0.]
 [ 0.  1.  0.  1.]
 [ 0.  1.  1.  0.]
 [ 0.  1.  1.  1.]
 [ 1. -0.  0.  0.]
 [ 1. -0.  0.  1.]]

ex_8_1) 모델 불러와 예측하기 2
import tensorflow as tf
from _7seg_data import X

model = tf.keras.models.load_model('model_1.h5')

x=X[:1]
print(x.shape)

Y=model.predict(x)
print(Y)

* 결과값
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(1, 7)
1/1 [==============================] - 0s 78ms/step
[[-0. -0.  0.  0.]]


ex_8) 딥러닝 활용 예제
import tensorflow as tf

mnist = tf.keras.datasets.fashion_mnist

(X, YT), (x, yt) = mnist.load_data()

X, x = X/255, x/255
X, x = X.reshape((60000,784)), x.reshape((10000,784)) # 데이터를 핌


model = tf.keras.Sequential([ 
    tf.keras.Input(shape=(784,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax'),
    ]) # 신경망 모양

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']) # 오차 함수, metrics 얼마나 맞추는지

model.fit(X,YT,epochs=5)

model.evaluate(x,yt) # predict도 사용가능


* 결과값
Epoch 5/5
1875/1875 [==============================] - 3s 2ms/step - loss: 0.2954 - accuracy: 0.8914
313/313 [==============================] - 0s 901us/step - loss: 0.3458 - accuracy: 0.8772

# tensorflow 예제
ex__)
import tensorflow as tf

mnist = tf.keras.datasets.fashion_mnist
mnist = tf.keras.datasets.cifar100
mnist = tf.keras.datasets.mnist

(X,YT), (x,yt) = mnist.load_data()

print(X.shape, YT.shape, x.shape, yt.shape)

import matplotlib.pyplot as plt
plt.imshow(X[3])
plt.show()

print(YT[0])
print(max(YT))
print(tf.one_hot(YT[0],100))

for row in range(28):
    for col in range(28):
        print('%4s' %X[0][row][col],end='')
    print()

plt.figure(figsize=(10,10))
for i in range(100):
    plt.subplot(5,20,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(X[i])
    plt.xlabel(YT[i])
plt.show()

X, x = X/255, x/255
X, x = X,reshape((60000,28*28)), x,reshape((10000,28*28))

model = tf.keras.Sequential([
    tf.keras.Input(shape = (28*28,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
    ])

model.compile(optimzer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X,YT,epochs=5) # 학습 횟수

model.evaluate(x,yt) # predict도 사용가능

