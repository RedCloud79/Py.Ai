## softmax 함수 분모 크기 줄이기

ex_2) 가장 큰 값을 빼서 구하기 softmax
(e(1.3-5.1),e(5.1-5.1),e(2.2-5.1),e(0.7-5.1),e(1.1-5.1),)
결과값 : (0.0223707718561656, 1.0, 0.05502322005640726, 0.012277339903068448, 0.01831563888873419)
sumY = (e(1.3-5.1)+e(5.1-5.1)+e(2.2-5.1)+e(0.7-5.1)+e(1.1-5.1))
sumY
결과값 : 1.1079869707043757
(e(1.3-5.1)/sumY,e(5.1-5.1)/sumY,e(2.2-5.1)/sumY,e(0.7-5.1)/sumY,e(1.1-5.1)/sumY)
결과값 : (0.020190464732580682, 0.9025376890165725, 0.049660529871960124, 0.011080761983386346, 0.01653055439550022)

# cross entropy 오차 구현해 보기
- softmax가 오면 무조껀 같이 오는 함수 이다.

ex_1)
from math import log as ln
-1*ln(0.9)
결과값 : 0.10536051565782628
-1*ln(0.99)
결과값 : 0.01005033585350145
-1*ln(0.999)
결과값 : 0.0010005003335835344
-1*ln(0.9999)
결과값 : 0.00010000500033334732
-1*ln(0.99999)
결과값 : 1.0000050000287824e-05


ex_2) crossentropy와 softmax예제
import tensorflow as tf
import numpy as np

X=np.array([[.05,.10]]) # 입력데이터 <=
YT=np.array([[0,1]]) # 목표데이터(라벨)
W=np.array([[.15,.25],[.20,.30]]) # 가중치 <=
B=np.array([.35,.35]) # 편향
W2=np.array([[.40,.50],[.45,.55]]) # 가중치 <=
B2=np.array([.60,.60]) # 편향

model=tf.keras.Sequential([
    tf.keras.Input(shape=(2,)),
    tf.keras.layers.Dense(2, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
]) # 신경망 모양 결정(W, B 내부적 준비)

model.layers[0].set_weights([W,B])
model.layers[1].set_weights([W2,B2])

model.compile(optimizer='sgd',# 7공식, 학습
                loss='categorical_crossentropy') # 2공식, 오차계산

Y=model.predict(X)
print(Y)

model.fit(X,YT,epochs=100000)

print('W=',model.layers[0].get_weights()[0])
print('B=',model.layers[0].get_weights()[1])
print('W2=',model.layers[1].get_weights()[0])
print('B2=',model.layers[1].get_weights()[1])

Y=model.predict(X)
print(Y)
* 결과값
W= [[0.20979609 0.31164446]
 [0.31959233 0.42337626]]
B= [1.5460793 1.5839566]
W2= [[-0.6355431   1.5354836 ]
 [-0.62160754  1.6215547 ]]
B2= [-0.7672052  1.9672418]
1/1 [==============================] - 0s 16ms/step
[[5.1897896e-05 9.9994814e-01]]






















